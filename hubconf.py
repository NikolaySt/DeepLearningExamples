import torch
from torch.nn.utils.rnn import pad_sequence
import numpy as np
import urllib.request
import os
import sys
from types import SimpleNamespace

from FastPitch import models
from FastPitch.common.text.text_processing import TextProcessing
from FastPitch.pitch_transform import pitch_transform_custom
from FastPitch.waveglow.denoiser import Denoiser


def _load_checkpoint(path):
    ckpt = torch.load(path, map_location="cpu")
    state_dict = ckpt['state_dict']
    if _checkpoint_from_distributed(state_dict):
        state_dict = _unwrap_distributed(state_dict)

    return state_dict, ckpt['config']


def _text_to_batch(lines,
                   device,
                   symbol_set="english_basic",
                   text_cleaners=["english_cleaners"],
                   batch_size=128):
    columns = ['text']
    fields = [lines]
    fields = {c: f for c, f in zip(columns, fields)}

    tp = TextProcessing(symbol_set, text_cleaners)

    fields['text'] = [
        torch.LongTensor(tp.encode_text(text)) for text in fields['text']
    ]
    order = np.argsort([-t.size(0) for t in fields['text']])

    fields['text'] = [fields['text'][i] for i in order]
    fields['text_lens'] = torch.LongTensor([t.size(0) for t in fields['text']])

    # cut into batches & pad
    batches = []
    for b in range(0, len(order), batch_size):
        batch = {f: values[b:b + batch_size] for f, values in fields.items()}
        for f in batch:
            if f == 'text':
                batch[f] = pad_sequence(batch[f], batch_first=True)
            if type(batch[f]) is torch.Tensor:
                batch[f] = batch[f].to(device)
        batches.append(batch)

    return batches[0]


def _build_pitch_transformation(custom=False,
                                flatten=False,
                                invert=False,
                                amplify=1.0,
                                shift_hz=0.0):

    if custom:

        def custom_(pitch, pitch_lens, mean, std):
            return (pitch_transform_custom(pitch * std + mean, pitch_lens) -
                    mean) / std

        return custom_

    fun = 'pitch'
    if flatten:
        fun = f'({fun}) * 0.0'
    if invert:
        fun = f'({fun}) * -1.0'
    if amplify:
        fun = f'({fun}) * {amplify}'
    if shift_hz != 0.0:
        fun = f'({fun}) + {shift_hz} / std'
    return eval(f'lambda pitch, pitch_lens, mean, std: {fun}')


def _denoiser(waveglow,
              filter_length=1024,
              n_overlap=4,
              win_length=1024,
              mode='zeros'):
    denoiser = Denoiser(waveglow, filter_length, n_overlap, win_length, mode)
    return denoiser


def _post_process(audios, mel_lens, fade_out=10, stft_hop_length=256):
    result = []
    for i, audio in enumerate(audios):
        audio = audio[:mel_lens[i].item() * stft_hop_length]

        if fade_out:
            fade_len = fade_out * stft_hop_length
            fade_w = torch.linspace(1.0, 0.0, fade_len)
            audio[-fade_len:] *= fade_w.to(audio.device)

        audio = audio / torch.max(torch.abs(audio))
        result.append(audio)
    return result


def _checkpoint_from_distributed(state_dict):
    """
    Checks whether checkpoint was generated by DistributedDataParallel. DDP
    wraps model in additional "module.", it needs to be unwrapped for single
    GPU inference.
    :param state_dict: model's state dict
    """
    ret = False
    for key, _ in state_dict.items():
        if key.find('module.') != -1:
            ret = True
            break
    return ret


def _unwrap_distributed(state_dict):
    """
    Unwraps model from DistributedDataParallel.
    DDP wraps model in additional "module.", it needs to be removed for single
    GPU inference.
    :param state_dict: model's state dict
    """
    new_state_dict = {}
    for key, value in state_dict.items():
        new_key = key.replace('module.1.', '')
        new_key = new_key.replace('module.', '')
        new_state_dict[new_key] = value
    return new_state_dict


def _download_checkpoint(checkpoint, force_reload):
    model_dir = os.path.join(torch.hub._get_torch_home(), 'checkpoints')
    if not os.path.exists(model_dir):
        os.makedirs(model_dir)
    ckpt_file = os.path.join(model_dir, os.path.basename(checkpoint))
    if not os.path.exists(ckpt_file) or force_reload:
        sys.stderr.write('Downloading checkpoint from {}\n'.format(checkpoint))
        urllib.request.urlretrieve(checkpoint, ckpt_file)
    return ckpt_file


def nvidia_fastpitch(device, checkpoint_path=None, symbol_set="english_basic"):
    if checkpoint_path == None:
        url = 'https://orionscloud.blob.core.windows.net/bb1e7e62-03a5-4d90-b15a-abb60ad55250/Checkpoints/nvidia_fastpitch_fp16_20210323.pt'
        checkpoint_path = _download_checkpoint(checkpoint=url,
                                               force_reload=False)

    state_dict, checkpoint_config = _load_checkpoint(checkpoint_path)
    fastpitch_arg = SimpleNamespace(**checkpoint_config)
    fastpitch_arg.symbol_set = symbol_set

    model_config = models.get_model_config("FastPitch", fastpitch_arg)

    fastpitch = models.get_model("FastPitch",
                                 model_config,
                                 device,
                                 forward_is_infer=True,
                                 jitable=False)

    status = ' ' + str(fastpitch.load_state_dict(state_dict, strict=True))
    print(f'Loaded {checkpoint_path} {status}')

    fastpitch.denoiser = _denoiser
    fastpitch.text_to_batch = _text_to_batch
    fastpitch.post_process = _post_process
    fastpitch.build_pitch_transformation = _build_pitch_transformation

    return fastpitch